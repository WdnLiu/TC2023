{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gHPcg59zjr6"
      },
      "source": [
        "Theory of Computation 2023 - Horacio Saggion\n",
        "\n",
        "Deliverable Regular Expressions\n",
        "\n",
        "\n",
        "Please indicate the full names and NIAs of the team members as well as the team number\n",
        "\n",
        "TEAM: 19\n",
        "\n",
        "MEMBERS: 254748, 252074\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# reading some data using the pandas library\n",
        "import pandas as pd\n",
        "\n",
        "my_data=pd.read_csv('DATA/ToC_2023_REs.csv', sep=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# the data is from twitter so it will contain interesting content\n",
        "# we only need the column 'text' to work with\n",
        "twitter_data=my_data['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qgurvr6NA7W"
      },
      "outputs": [],
      "source": [
        "# (1) TODO print 10 lines of the data to understand what type of text we are working with\n",
        "ten_lines = twitter_data.head(10)\n",
        "\n",
        "print(ten_lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWqVvz4x0t_G"
      },
      "source": [
        "(2) TODO: Understanding Regular\n",
        "\n",
        "Using the notation used in theory write regular expressions for\n",
        "\n",
        "\n",
        "1. zero or more 'a'\n",
        "2. one or more 'a'\n",
        "1. starts with a and it is followed by zero or more b's\n",
        "2. a sequence of one or more digits\n",
        "2. two digits followed by a - followed by two digits\n",
        "1. string of a's of odd length\n",
        "2. string of a's of even length\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. $(a)^{*}$\n",
        "2. $(a)^{+}$\n",
        "3. $a(b)^{*}$\n",
        "4. $(1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 0)^{+}$\n",
        "5. $(1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 0)^{2}a(1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 0)^{2}$\n",
        "6. $a(aa)^{*}$\n",
        "7. $(aa)^{*}$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDJhBF85JwUs"
      },
      "outputs": [],
      "source": [
        "# (3)  TODO: Run the following code comment what the program is doing\n",
        "\n",
        "import re\n",
        "\n",
        "my_text =  ''' \"aaa bbb abcd yz\n",
        "\n",
        "123 4567\n",
        "\n",
        "\n",
        "2   4 5 7 8 90\n",
        "\n",
        "Abc dEFG XYZ\n",
        "\" '''\n",
        "\n",
        "#  What are we searching for?\n",
        "#  re.compile looks for any sequence of two letters (upper or lower case) and stores them as objects that can be operated on using methods.\n",
        "#  In the case of abcd we have two combinations in a row, the program will take the last sequence that satisfies our demand.\n",
        "\n",
        "my_regex=re.compile(r'(a-zA-Z][a-zA-Z])+')\n",
        "\n",
        "result=my_regex.findall(my_text)\n",
        "\n",
        "\n",
        "\n",
        "print(result)\n",
        "\n",
        "# TODO: What are we searching for?\n",
        "# re.compile looks for any sequence of two uppercase letters and stores them as objects that can be operated on using methods.\n",
        "\n",
        "my_regex=re.compile(r'([A-Z][A-Z])+')\n",
        "\n",
        "result=my_regex.findall(my_text)\n",
        "\n",
        "print(result)\n",
        "\n",
        "#  TODO: What are we searching for?\n",
        "# re.compile looks for any sequence of one or more uppercase letters and stores them as objects that can be operated on using methods.\n",
        "\n",
        "my_regex=re.compile(r'([A-Z][A-Z]*)')\n",
        "\n",
        "result=my_regex.findall(my_text)\n",
        "\n",
        "print(result)\n",
        "\n",
        "\n",
        "\n",
        "# TODO: What is the difference with the above?\n",
        "#The following uses re.search, which finds the first occurrence of the input string, instead of printing the strings that satisfy the condition\n",
        "#it prints whether it has matched, the span, which is a tuple containing the starting index and the last index of the coincidence and the string matched.\n",
        "\n",
        "result = re.search(r'([A-Z][A-Z]*)', my_text)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dpm1qvtjJMP1"
      },
      "source": [
        "TODO: Take the tutorial  at [W3Schools on regular expressions in Python](https://www.w3schools.com/python/python_regex.asp)\n",
        "and practice the code.*texto en cursiva*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfXmYXThJlNi"
      },
      "source": [
        "(4) TODO: Answer the following questions about RE in python\n",
        "\n",
        "\n",
        "\n",
        "1.  What is the purpose of the findall function\n",
        "2.  What is the pupose of the search function\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. findall() returns a list with all the matches.\n",
        "2. search() looks for strings that match the condition and returns a Match Object in case it founds one. In case it founds more than one match it will just take the first one. Returns None if it does not find a match. The return Match has a tuple span that contains the starting and last index of the match, and the string of the match.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dh6Tz0KTOowX"
      },
      "outputs": [],
      "source": [
        "# (5) TODO: examine item 95 of the data\n",
        "item95 = twitter_data[95]\n",
        "print(item95)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBZpOMGJPk3n"
      },
      "outputs": [],
      "source": [
        "# (6) TODO: list sentences with string 2030 in the first 100 sentences of your data using a regular expression\n",
        "first100 = twitter_data.head(100)\n",
        "\n",
        "for i in first100:\n",
        "    if re.search(r\"2030\", i) != None:\n",
        "        print(i)\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amGGdho6QT9x"
      },
      "outputs": [],
      "source": [
        "# (7) TODO: list sentences with string 2030 in the first 100 sentences of your data\n",
        "# this time 2030 should be a full word not part of a word\n",
        "\n",
        "first100= twitter_data.head(100)\n",
        "\n",
        "for i in first100:\n",
        "    if re.search(r\"\\b2030\", i) != None:\n",
        "        print(i)\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9V1euKcIRRx-"
      },
      "outputs": [],
      "source": [
        "# (8) TODO:  find all sentences, within the first 100 sentences,  containing  a twitter hash tag '#' using regular expressions\n",
        "first100= twitter_data.head(100)\n",
        "\n",
        "for i in first100:\n",
        "    if re.search(\"#\", i) != None:\n",
        "        print(i)\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijoQCwsYctYe"
      },
      "outputs": [],
      "source": [
        "# install library NLTK to  work with texts\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7v1f7hMdC1g"
      },
      "outputs": [],
      "source": [
        "# import stopwords and punctuation for English\n",
        "import nltk\n",
        "stops=nltk.download('stopwords')\n",
        "punkt=nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aHDvPamciXx"
      },
      "outputs": [],
      "source": [
        "# word tokenization with nltk\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"Oh!!! I can't believe it is Friday.\"\n",
        "print(word_tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZRkwjJddl_W"
      },
      "outputs": [],
      "source": [
        "# (9) TODO tokenizing text vs tokenizing tweets\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tt = TweetTokenizer()\n",
        "# produce a tokenization using word_tokenize(...) and a tokenization using the tweet tokenizer ( TweetTokenizer() )\n",
        "print(\"word_tokenize: \", word_tokenize(twitter_data[0]))\n",
        "print()\n",
        "# print(twitter_data[0])\n",
        "print(\"TweetTokenizer: \", tt.tokenize(twitter_data[0]))\n",
        "print()\n",
        "\n",
        "# How are they different?\n",
        "# Word_tokenizer returns the set of words in every tweet splitting the special characters like \"@\" or \"#\" and links. On the other hand\n",
        "# TweetTokenizer splits every word and does not separate this special characters from the following word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvqeoFnAfVo8"
      },
      "outputs": [],
      "source": [
        "#  (10) TODO: what do you get if you tokenize item 95 of the data, show the tokens\n",
        "tt = TweetTokenizer()\n",
        "print(tt.tokenize(twitter_data[95]))\n",
        "\n",
        "# It returns the set of tokens that is printed afterwards. The main difference is the appearence \"emojis\" that are splitted from the rest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Co2_FitMhWRs"
      },
      "outputs": [],
      "source": [
        "# (11) TODO: using a regular expression list  the emojis ☛   ♀  or ♂  and the position in which the occur\n",
        "# You should use the UNICODE representation of such emojis, which you can figure out checking a table or using\n",
        "# print(format(ord(CHARACTER), '#08x'))\n",
        "\n",
        "#Instance of TweetTokenizer\n",
        "tt = TweetTokenizer()\n",
        "\n",
        "#We will tokenize every tweet, then search for emojis, and print the position of the token following the format:\n",
        "#Matches\n",
        "#Tweet\n",
        "#Tokens\n",
        "#We are assuming that for every sentence there could be more than one occurrence of each emoji\n",
        "\n",
        "for i in twitter_data:\n",
        "    tweet_tokens = tt.tokenize(i)\n",
        "    matched = False\n",
        "\n",
        "    for token in range(len(tweet_tokens)):\n",
        "        result1 = re.match(\"\\U0000261B\", tweet_tokens[token])\n",
        "        result2 = re.match(\"\\U00002642\", tweet_tokens[token])\n",
        "        result3 = re.match(\"\\U00002640\", tweet_tokens[token])\n",
        "        \n",
        "        if (result1 != None or result2 != None or result3 != None):\n",
        "            matched = True\n",
        "        if result1 != None:\n",
        "            print(f\"Matched \\U0000261B at token {token}\")\n",
        "        elif result2 != None:\n",
        "            print(f\"Matched \\U00002642 at token {token}\")\n",
        "        elif result3 != None:\n",
        "            print(f\"Matched \\U00002640 at token {token}\")\n",
        "    if matched:\n",
        "        print(i)\n",
        "        print(tweet_tokens)\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nav-_y5jb5sf"
      },
      "outputs": [],
      "source": [
        "# (12) TODO: extract all hashtags in first 100 sentences\n",
        "first100= twitter_data.head(100)\n",
        "tt = TweetTokenizer()\n",
        "\n",
        "for i in first100:\n",
        "    tweet_tokens = tt.tokenize(i)\n",
        "    matched = False\n",
        "\n",
        "    for token in range(len(tweet_tokens)):\n",
        "        result1 = re.match(\"#\", tweet_tokens[token])\n",
        "        if result1 != None:\n",
        "            matched = True\n",
        "            print(tweet_tokens[token])\n",
        "\n",
        "    if matched:\n",
        "        print(i)\n",
        "        print(tweet_tokens)\n",
        "        print()\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmRllXHWTSDj"
      },
      "outputs": [],
      "source": [
        "# (13) TODO: extract only the TAG of the hashtag in the first 100 sentences\n",
        "from nltk.tokenize import word_tokenize\n",
        "first100= twitter_data.head(100)\n",
        "\n",
        "for i in first100:\n",
        "    word_tokens = word_tokenize(i)\n",
        "    \n",
        "    matched = False\n",
        "\n",
        "    for token in range(len(tweet_tokens)):\n",
        "        result1 = re.match(\"#\", word_tokens[token])\n",
        "        if result1 != None:\n",
        "            matched = True\n",
        "            print(word_tokens[token])\n",
        "\n",
        "    if matched:\n",
        "        print(i)\n",
        "        print(word_tokens)\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NdPPFQCf3EM"
      },
      "outputs": [],
      "source": [
        "# (14) TODO: list all hashtags which include a year (something that looks like YYYY)\n",
        "tt = TweetTokenizer()\n",
        "\n",
        "for i in twitter_data:\n",
        "    tweet_tokens = tt.tokenize(i)\n",
        "    matched = False\n",
        "\n",
        "    for token in range(len(tweet_tokens)):\n",
        "        result1 = re.match(\"#[a-zA-Z]*([0-9][0-9][0-9][0-9])\", tweet_tokens[token])\n",
        "        if result1 != None:\n",
        "            matched = True\n",
        "            print(tweet_tokens[token])\n",
        "\n",
        "    if matched:\n",
        "        print(i)\n",
        "        print(tweet_tokens)\n",
        "        print()\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WipAUhfUCT9"
      },
      "outputs": [],
      "source": [
        "# (15) TODO: for all hashtags including a year, extract the year\n",
        "from nltk.tokenize import word_tokenize\n",
        "tt = TweetTokenizer()\n",
        "years = []\n",
        "\n",
        "for i in twitter_data:\n",
        "    tweet_tokens = tt.tokenize(i)\n",
        "    matched = False\n",
        "    sentence = []\n",
        "    for token in range(len(tweet_tokens)):\n",
        "        result1 = re.match(\"#[a-zA-Z]*([0-9][0-9][0-9][0-9])\", tweet_tokens[token])\n",
        "        if result1 != None:\n",
        "            matched = True\n",
        "\n",
        "            year = re.findall(\"[0-9][0-9][0-9][0-9]\", tweet_tokens[token])\n",
        "            \n",
        "            print(year)\n",
        "            sentence.append(year[0])\n",
        "            \n",
        "\n",
        "    if matched:\n",
        "        print(i)\n",
        "        print(tweet_tokens)\n",
        "        print()\n",
        "        years.append(sentence)\n",
        "\n",
        "print(years)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05SpXJ5QoSlD"
      },
      "outputs": [],
      "source": [
        "# (16) TODO:  for all hashtags including a year, extract the year, list each year once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "694J_vwO4IUF"
      },
      "outputs": [],
      "source": [
        "# (17) TODO: find all user mentions in the first 1000 sentences\n",
        "\n",
        "tt = TweetTokenizer()\n",
        "first1000= twitter_data.head(1000)\n",
        "for i in first1000:\n",
        "    tweet_tokens = tt.tokenize(i)\n",
        "    matched = False\n",
        "\n",
        "    for token in range(len(tweet_tokens)):\n",
        "        result1 = re.match(\"@[a-zA-Z]*\", tweet_tokens[token])\n",
        "        if result1 != None:\n",
        "            matched = True\n",
        "            print(tweet_tokens[token])\n",
        "\n",
        "    if matched:\n",
        "        print(i)\n",
        "        print(tweet_tokens)\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMV6Sbjv4mxL"
      },
      "outputs": [],
      "source": [
        "#  (18) TODO: find all user mentions such that contain UPPERCASE letters only, in the first 1000 sentences\n",
        "\n",
        "tt = TweetTokenizer()\n",
        "first1000= twitter_data.head(1000)\n",
        "for i in first1000:\n",
        "    tweet_tokens = tt.tokenize(i)\n",
        "    matched = False\n",
        "\n",
        "    for token in range(len(tweet_tokens)):\n",
        "        result1 = re.match(\"@[A-Z]*\", tweet_tokens[token])\n",
        "        if result1 != None:\n",
        "            matched = True\n",
        "            print(tweet_tokens[token])\n",
        "\n",
        "    if matched:\n",
        "        print(i)\n",
        "        print(tweet_tokens)\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ruZ9sM28tsQ"
      },
      "outputs": [],
      "source": [
        "# (19) TODO: find all strings containing the substring UN\n",
        "tt = TweetTokenizer()\n",
        "for i in twitter_data:\n",
        "    tweet_tokens = tt.tokenize(i)\n",
        "    matched = False\n",
        "\n",
        "    for token in range(len(tweet_tokens)):\n",
        "        result1 = re.match(\"[a-zA-Z]*UN[a-zA-Z]*\", tweet_tokens[token])\n",
        "        if result1 != None:\n",
        "            matched = True\n",
        "            print(tweet_tokens[token])\n",
        "\n",
        "    if matched:\n",
        "        print(i)\n",
        "        print(tweet_tokens)\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLZ_qxcw9aRP"
      },
      "outputs": [],
      "source": [
        "# (20)  TODO: find all strings containing the substring UN strictly in the middle of the token\n",
        "tt = TweetTokenizer()\n",
        "for i in twitter_data:\n",
        "    tweet_tokens = tt.tokenize(i)\n",
        "    matched = False\n",
        "\n",
        "    for token in range(len(tweet_tokens)):\n",
        "        result1 = re.match(\"[a-zA-Z]+UN[a-zA-Z]+\", tweet_tokens[token])\n",
        "        if result1 != None:\n",
        "            matched = True\n",
        "            print(tweet_tokens[token])\n",
        "\n",
        "    if matched:\n",
        "        print(i)\n",
        "        print(tweet_tokens)\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nx-d8JmW9t-K"
      },
      "outputs": [],
      "source": [
        "# (21) TODO: find all strings containing just numbers\n",
        "tt = TweetTokenizer()\n",
        "first1000= twitter_data.head(1000)\n",
        "for i in twitter_data:\n",
        "    tweet_tokens = tt.tokenize(i)\n",
        "    matched = False\n",
        "\n",
        "    for token in range(len(tweet_tokens)):\n",
        "        result1 = re.match(\"^[0-9]+$\", tweet_tokens[token])\n",
        "        if result1 != None:\n",
        "            matched = True\n",
        "            print(tweet_tokens[token])\n",
        "\n",
        "    if matched:\n",
        "        print(i)\n",
        "        print(tweet_tokens)\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ekoRz_y-CCu"
      },
      "outputs": [],
      "source": [
        "# (22) TODO: find all strings containing no numbers\n",
        "tt = TweetTokenizer()\n",
        "words = []\n",
        "\n",
        "for i in twitter_data:\n",
        "    tweet_tokens = tt.tokenize(i)\n",
        "    matched = False\n",
        "    sentence_words = []\n",
        "\n",
        "    for token in range(len(tweet_tokens)):\n",
        "        result1 = re.match(\"^[^0-9]+$\", tweet_tokens[token])\n",
        "        if result1 != None:\n",
        "            matched = True\n",
        "            sentence_words.append(tweet_tokens[token])\n",
        "\n",
        "    if matched:\n",
        "        print(sentence_words)\n",
        "        print(i)\n",
        "        print(tweet_tokens)\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s42UbWXEDTxI"
      },
      "outputs": [],
      "source": [
        "# (23) TODO: find any string between double quotes in the sentences\n",
        "tt = TweetTokenizer()\n",
        "for i in twitter_data:\n",
        "    result = re.findall('\"(.*?)\"', i)\n",
        "    if result != None:\n",
        "        print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQlNBZDSE_R5"
      },
      "outputs": [],
      "source": [
        "# (24) TODO: extract all urls in the 100 first sentences\n",
        "tt = TweetTokenizer()\n",
        "for i in twitter_data:\n",
        "    tweet_tokens = tt.tokenize(i)\n",
        "    matched = False\n",
        "\n",
        "    for token in range(len(tweet_tokens)):\n",
        "        result1 = re.match(\"https://\", tweet_tokens[token])\n",
        "        if result1 != None:\n",
        "            matched = True\n",
        "            print(tweet_tokens[token])\n",
        "\n",
        "    if matched:\n",
        "        print(i)\n",
        "        print(tweet_tokens)\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhi6l_M7HWR5"
      },
      "outputs": [],
      "source": [
        "# (25) TODO: extract all dates (can be DD/MM/YY or DD/MM/YYYY)\n",
        "tt = TweetTokenizer()\n",
        "for i in twitter_data:\n",
        "    tweet_tokens = tt.tokenize(i)\n",
        "    matched = False\n",
        "\n",
        "    for token in range(len(tweet_tokens)):\n",
        "        result1 = re.match(\"[0-3][0-9]/[0-1][0-9]/[0-9][0-9]\", tweet_tokens[token])\n",
        "        result2 = re.match(\"[0-3][0-9]/[0-1][0-9]/[0-9][0-9][0-9][0-9]\", tweet_tokens[token])\n",
        "        if result1 != None or result2 != None:\n",
        "            matched = True\n",
        "            print(tweet_tokens[token])\n",
        "\n",
        "    if matched:\n",
        "        print(i)\n",
        "        print(tweet_tokens)\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mguuUvi296x"
      },
      "outputs": [],
      "source": [
        "# (26) TODO: consider the following list of UNICODEs for emojis\n",
        "emojis=['\\U0001F603', '\\U0001F604', '\\U0001F601', '\\U0001F606', '\\U0001F605', '\\U0001F923',\n",
        "\t\t'\\U0001F602', '\\U0001F642', '\\U0001F643', '\\U0001F609', '\\U0001F60A', '\\U0001F607']\n",
        "\n",
        "# print them to understand what they are\n",
        "\n",
        "print(emojis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dydH4Gcl6hR6"
      },
      "outputs": [],
      "source": [
        "# (27)  TODO: use regular expression to find and list any sentence containing at least one of the emojis above\n",
        "tt = TweetTokenizer()\n",
        "\n",
        "for i in twitter_data:\n",
        "    for emoji in emojis:\n",
        "        result = re.search(emoji, i)\n",
        "        if result != None:\n",
        "            print(i)\n",
        "            print()\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XnwDAHtXrFo"
      },
      "outputs": [],
      "source": [
        "# (28) TODO: use regular expressions to find and extract all ocurrences of  previous emojis and count how many of each\n",
        "# list the results\n",
        "\n",
        "dict_emojis = {\n",
        "    '\\U0001F603' : 0,\n",
        "    '\\U0001F604' : 0,\n",
        "    '\\U0001F601' : 0, \n",
        "    '\\U0001F606' : 0,\n",
        "    '\\U0001F605' : 0,\n",
        "    '\\U0001F923' : 0,\n",
        "\t'\\U0001F602' : 0,\n",
        "    '\\U0001F642' : 0,\n",
        "    '\\U0001F643' : 0,\n",
        "    '\\U0001F609' : 0,\n",
        "    '\\U0001F60A' : 0,\n",
        "    '\\U0001F607' : 0\n",
        "}\n",
        "counter = [0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "for i in twitter_data:\n",
        "    for emoji in emojis:\n",
        "        result = re.search(emoji, i)\n",
        "        if result != None:\n",
        "            dict_emojis[emoji] += 1\n",
        "            print(i)\n",
        "            print()\n",
        "            break\n",
        "\n",
        "print(dict_emojis)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMvup0uQaMRb"
      },
      "outputs": [],
      "source": [
        "# (29) TODO: identify any sentences with 2 emojis in sequence such as 😊😊\n",
        "emojis=['\\U0001F603', '\\U0001F604', '\\U0001F601', '\\U0001F606', '\\U0001F605', '\\U0001F923',\n",
        "\t\t'\\U0001F602', '\\U0001F642', '\\U0001F643', '\\U0001F609', '\\U0001F60A', '\\U0001F607']\n",
        "\n",
        "for i in twitter_data:\n",
        "    for emoji in emojis:\n",
        "        result = re.search(rf\"{emoji}{emoji}\", i)\n",
        "        if result != None:\n",
        "            print(i)\n",
        "            print()\n",
        "            break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stZ1-ioUJiJT"
      },
      "outputs": [],
      "source": [
        "# (30) TODO: identify any sentences with 3 emojis in sequence such as 😊😊\n",
        "\n",
        "emojis=['\\U0001F603', '\\U0001F604', '\\U0001F601', '\\U0001F606', '\\U0001F605', '\\U0001F923',\n",
        "\t\t'\\U0001F602', '\\U0001F642', '\\U0001F643', '\\U0001F609', '\\U0001F60A', '\\U0001F607']\n",
        "\n",
        "for i in twitter_data:\n",
        "    for emoji in emojis:\n",
        "        result = re.search(rf\"{emoji}{emoji}{emoji}\", i)\n",
        "        if result != None:\n",
        "            print(i)\n",
        "            print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
